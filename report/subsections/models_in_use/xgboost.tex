XGBoost (Extreme Gradient Boosting) is a powerful, scalable, and highly optimized machine learning algorithm used for both classification and regression tasks, though it excels in structured/tabular data.
It belongs to the family of gradient boosting methods, which build an ensemble of decision trees sequentially, where each tree corrects the errors of the previous ones.
XGBoost enhances gradient boosting with advanced regularization, parallel processing, and handling of missing data, making it a go-to choice in data science competitions and real-world applications.

The core idea is: Combine many weak decision trees (weak learners) into a strong predictive model by iteratively minimizing a loss function using gradient-based optimization.

\subsubsection{Basic Components}
\begin{itemize}
    \item \textbf{Decision Trees}: The base learners in XGBoost, typically shallow trees (e.g., depth 3â€“10) to prevent overfitting.
    \item \textbf{Ensemble}: A collection of trees whose predictions are combined (summed for regression, aggregated for classification).
    \item \textbf{Loss Function}: Measures the difference between predicted and actual values (e.g., mean squared error for regression, log loss for classification).
    \item \textbf{Regularization}: Penalties on tree complexity to prevent overfitting (e.g., L1/L2 regularization on leaf weights).
    \item \textbf{Gradient and Hessian}: First-order (gradient) and second-order (Hessian) derivatives of the loss function guide tree construction.
    \item \textbf{Boosting}: Sequential addition of trees, where each tree focuses on correcting the residuals (errors) of the previous trees.
    \item \textbf{Hyperparameters}: Parameters like learning rate, max depth, and regularization terms control model behavior.
\end{itemize}

\subsubsection{Detailed Algorithm}

\begin{enumerate}[label=Step \arabic*:, align=left, leftmargin=20pt,labelsep=1em]
    \item Data Preparation
    \begin{itemize}
        \item Prepare the dataset $D = \{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}$, where $x_i \in \mathbb{R}^d$ is a feature vector with d dimensions, and $y_i$ is a label (e.g., $y_i \in \{0, 1\}$ for binary classification, $y_i \in \mathbb{R}$ for regression).
        \item Handle missing values (XGBoost can automatically learn how to treat them).
        \item No feature scaling is required, as XGBoost is tree-based.
    \end{itemize}

    \item Initialize the Model
    \begin{itemize}
        \item Start with an initial prediction (e.g., mean of target values for regression, log-odds for classification).
        \item Define the loss function (e.g., MSE for regression, log loss for classification) and regularization terms.
    \end{itemize}

    \item Build Trees Sequentially
    \begin{itemize}
        \item For T iterations (number of trees):
        \begin{enumerate}
            \item Compute \textbf{gradients} (first derivative of the loss with respect to predictions) and \textbf{Hessians} (second derivative) for each sample.
            \item Build a decision tree to fit the gradients, using a specialized objective function that balances loss reduction and tree complexity.
            \item Update the model's predictions by adding the new tree's output, scaled by a \textbf{learning rate} ($\eta$).
        \end{enumerate}
        \item Each tree focuses on the residuals (errors) of the current model.
    \end{itemize}

    \item Prediction
    \begin{itemize}
        \item For a new input $x$:
        \begin{itemize}
            \item Sum the predictions from all trees (for regression) or compute a weighted sum and apply a sigmoid function (for classification).
            \item Output the final class or value.
        \end{itemize}
    \end{itemize}
\end{enumerate}

\subsubsection{Mathematical Formulas}

XGBoost's mathematics combines \textbf{gradient boosting}, \textbf{decision tree construction}, and \textbf{regularized optimization}.
Below are the key formulas, explained intuitively.

\smallskip
\textbf{Objective Function}

The goal is to minimize a loss function plus regularization:
\[\text{Obj} = \sum_{i=1}^N \ell(y_i, \hat{y}_i) + \sum_{t=1}^T \Omega(f_t)\]

\begin{itemize}
    \item $\ell(y_i, \hat{y}_i)$: Loss function measuring the error between true label $y_i$ and prediction $\hat{y}_i$.
    \begin{itemize}
        \item Regression (MSE):
        \[
            \ell(y_i, \hat{y}_i) = \frac{1}{2}(y_i - \hat{y}_i)^2
        \]
        \item Classification (Log Loss):
        \[
            \ell(y_i, \hat{y}_i) = -[y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
        \]
    \end{itemize}

    \item $\Omega(f_t)$: Regularization term for tree $t$:
    \[
        \Omega(f_t) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2 + \alpha \sum_{j=1}^T |w_j|
    \]
    \begin{itemize}
        \item $T$: Number of leaves in the tree.
        \item $w_j$: Leaf weight (output value of leaf $j$).
        \item $\gamma$: Penalty for adding leaves (controls tree size).
        \item $\lambda$: L2 regularization on leaf weights.
        \item $\alpha$: L1 regularization on leaf weights.
    \end{itemize}
\end{itemize}

\smallskip
\textbf{Gradient Boosting}

Predictions are the sum of outputs from T trees:
\[
    \hat{y}_i = \sum_{t=1}^T f_t(x_i)
\]
where $f_t(x_i)$ is the output of tree $t$ for sample $x_i$.

At iteration t, the prediction is:
\[
    \hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + \eta f_t(x_i)
\]

\begin{itemize}
    \item $\eta$: Learning rate (shrinks the contribution of each tree to prevent overfitting).
\end{itemize}

\smallskip
\textbf{Gradient and Hessian}

For each sample i, compute:
\begin{itemize}
    \item Gradient (first derivative): $g_i = \frac{\partial \ell(y_i, \hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)}}$
    \begin{itemize}
        \item For MSE: $g_i = \hat{y}_i^{(t-1)} - y_i$.
        \item For log loss: $g_i = \hat{y}_i^{(t-1)} - y_i$, where $\hat{y}_i^{(t-1)}$ is the predicted probability.
    \end{itemize}
    \medskip
    \item Hessian (second derivative): $h_i = \frac{\partial^2 \ell(y_i, \hat{y}_i^{(t-1)})}{\partial (\hat{y}_i^{(t-1)})^2}$
    % Nested list for specific cases
    \begin{itemize}
        \item For MSE: $h_i = 1$.
        \item For log loss: $h_i = \hat{y}_i^{(t-1)} (1 - \hat{y}_i^{(t-1)})$.
    \end{itemize}
\end{itemize}

These guide the tree to focus on samples with larger errors.

\textbf{Tree Construction}

Each tree is built to minimize:
\[
    \text{Obj}^{(t)} = \sum_{i=1}^N [g_i f_t(x_i) + \frac{1}{2} h_i f_t(x_i)^2] + \Omega(f_t)
\]

\textbf{Intuition:} The tree predicts values that reduce the loss (via gradients) while keeping the tree simple (via regularization).

\begin{itemize}
    \item For a leaf $j$ with samples $I_j$, the optimal leaf weight is:
    \[
        w_j^* = -\frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}
    \]

    \item The gain from splitting a node into left ($I_L$) and right ($I_R$) branches is:
    \[
        \text{Gain} = \frac{1}{2} \left[ \frac{(\sum_{i \in I_L} g_i)^2}{\sum_{i \in I_L} h_i + \lambda} + \frac{(\sum_{i \in I_R} g_i)^2}{\sum_{i \in I_R} h_i + \lambda} - \frac{(\sum_{i \in I} g_i)^2}{\sum_{i \in I} h_i + \lambda} \right] - \gamma
    \]
    \begin{itemize}
        \item Split if Gain $> 0$, choosing the feature and threshold that maximizes Gain.
    \end{itemize}
\end{itemize}

\textbf{Prediction}

\begin{itemize}
    \item Regression:
    \[
        \hat{y}_i = \sum_{t=1}^T f_t(x_i)
    \]

    \item Classification (Binary):
    \[
        \hat{y}_i = \sigma \left( \sum_{t=1}^T f_t(x_i) \right)
    \]
    where $\sigma(z) = \frac{1}{1+e^{-z}}$ is the sigmoid function for probability output.
\end{itemize}

\subsubsection{Advantages}
\begin{itemize}
    \item High Accuracy: Often outperforms other models on structured data due to sequential error correction.
    \item High Accuracy: Often outperforms other models on structured data due to sequential error correction.
    \item Handles Missing Data: Automatically learns how to treat missing values.
    \item Regularization: Prevents overfitting with L1/L2 penalties and tree pruning.
    \item Scalable: Optimized for speed with parallel processing and efficient tree construction.
    \item Feature Importance: Provides insights into which features drive predictions.
\end{itemize}

\subsubsection{Disadvantages}
\begin{itemize}
    \item Computationally Intensive: Training can be slow for large datasets with many trees.
    \item Complex Tuning: Requires careful tuning of hyperparameters (e.g., learning rate, max depth).
    \item Less Interpretable: Ensemble of trees is harder to interpret than a single tree.
    \item Poor with Sparse Data: Less effective for high-dimensional, sparse data (e.g., text) compared to Naive Bayes.
    \item Overfitting Risk: Without proper regularization, can overfit noisy data.
\end{itemize}